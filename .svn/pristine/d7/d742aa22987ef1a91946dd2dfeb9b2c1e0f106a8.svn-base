---
name: AI Language Tutoring Service
overview: Build a browser-based AI language tutoring service with voice conversations, pronunciation analysis, translation features, and highly customizable UI. Using Kokoro TTS for free local voice generation, Web Speech API for transcription, and OpenAI GPT for conversation and analysis.
todos:
  - id: setup
    content: Initialize Next.js project with TypeScript, Tailwind, shadcn/ui
    status: pending
  - id: database
    content: Setup SQLite with Drizzle ORM, create schema and migrations
    status: pending
  - id: tts
    content: Integrate Kokoro TTS with voice selection and speed control
    status: pending
  - id: stt
    content: Implement Web Speech API wrapper for voice recording
    status: pending
  - id: openai
    content: Setup OpenAI client with chat and analysis endpoints
    status: pending
  - id: translation
    content: Integrate DeepL API for word/phrase translation
    status: pending
  - id: chat-ui
    content: Build chat interface with message bubbles, recording, playback
    status: pending
  - id: analysis-ui
    content: Create message analysis popup component
    status: pending
  - id: settings
    content: Implement settings page with all customization options
    status: pending
  - id: prompts
    content: Design customizable AI prompt system for different conversation types
    status: pending
  - id: tts-plugin
    content: Create TTS provider plugin system with interface for adding new engines
    status: pending
  - id: ai-plugin
    content: Create AI provider plugin system supporting cloud and local models
    status: pending
  - id: context-manager
    content: Implement conversation context manager with sliding window and summarization
    status: pending
  - id: summarization
    content: Implement local summarization with WebLLM/Ollama fallback to cloud
    status: pending
isProject: false
---

# AI Language Tutoring Service (Lanqua)

## Architecture Overview

```mermaid
flowchart TB
    subgraph Frontend [Next.js Frontend]
        UI[React UI with Tailwind + shadcn/ui]
        AudioPlayer[Audio Player Component]
        Recorder[Voice Recorder Component]
        ChatUI[Chat Interface]
        Settings[Settings Panel]
    end
    
    subgraph LocalProcessing [Browser-Side Processing]
        KokoroTTS[Kokoro TTS - WebGPU]
        WebSpeech[Web Speech API - STT]
    end
    
    subgraph Backend [Next.js API Routes]
        ChatAPI[Chat API]
        AnalysisAPI[Analysis API]
        TranslationAPI[Translation API]
    end
    
    subgraph ExternalAPIs [External Services]
        OpenAI[OpenAI GPT-4o-mini]
        DeepL[DeepL API Free]
        WhisperOpt[Whisper API - Optional]
    end
    
    subgraph Storage [SQLite Database]
        Chats[Chats Table]
        Messages[Messages Table]
        UserSettings[Settings Table]
        Vocabulary[Saved Words Table]
    end
    
    UI --> ChatUI
    UI --> Settings
    ChatUI --> Recorder
    ChatUI --> AudioPlayer
    
    Recorder --> WebSpeech
    AudioPlayer --> KokoroTTS
    
    ChatUI --> ChatAPI
    ChatAPI --> OpenAI
    ChatAPI --> Storage
    
    ChatUI --> AnalysisAPI
    AnalysisAPI --> OpenAI
    
    ChatUI --> TranslationAPI
    TranslationAPI --> DeepL
```



## Technology Stack


| Layer       | Technology                          | Reason                                  |
| ----------- | ----------------------------------- | --------------------------------------- |
| Framework   | Next.js 14+ (App Router)            | Server components, API routes, great DX |
| UI          | React + Tailwind CSS + shadcn/ui    | Highly customizable, modern look        |
| Database    | SQLite + Drizzle ORM                | Simple, file-based, no external deps    |
| TTS         | Plugin System (default: Kokoro)     | Extensible, supports local + cloud      |
| STT         | Plugin System (default: Web Speech) | Extensible, supports local + cloud      |
| AI          | Plugin System (default: OpenAI)     | Extensible, supports local + cloud      |
| Translation | DeepL API Free                      | 500k chars/month free                   |
| State       | Zustand                             | Lightweight state management            |


### Available Providers (Out of the Box)

**TTS Providers:**

- Kokoro TTS (local, WebGPU) - FREE, natural voices
- Web Speech Synthesis (local, fallback) - FREE, basic quality
- OpenAI TTS (cloud) - $15/1M chars, high quality
- ElevenLabs (cloud) - ~$0.06/min, premium quality

**STT Providers:**

- Web Speech API (local) - FREE, browser-native
- OpenAI Whisper (cloud) - $0.006/min, high accuracy

**AI Providers:**

- OpenAI GPT-4o-mini (cloud) - $0.15/1M input, recommended
- OpenAI GPT-4o (cloud) - $2.50/1M input, premium
- Ollama (local server) - FREE, requires local setup
- WebLLM (local browser) - FREE, runs in browser via WebGPU

## Cost Analysis (Per User/Month)

- **Kokoro TTS**: $0 (runs locally in browser)
- **Web Speech API**: $0 (browser-native)
- **OpenAI GPT-4o-mini**: ~$0.50-2.00 (depending on usage, ~100 conversations)
- **DeepL Translation**: $0 (free tier: 500k chars)
- **Total estimated**: ~$0.50-2.00/active user/month

## Core Features Implementation

### 1. Chat System

- Create conversations with topic/roleplay/general modes
- Store chat history in SQLite
- Support multiple concurrent chats
- Customizable AI personality prompts

### 2. Voice Conversation Flow

**Key UX Rule: AI message text is revealed only AFTER audio playback completes.**
This forces the learner to practice listening comprehension first.

```mermaid
sequenceDiagram
    participant User
    participant UI as Chat UI
    participant TTS as TTS Provider
    participant WebSpeech as Web Speech API
    participant API as Backend API
    participant GPT as AI Provider

    User->>UI: Creates new chat with topic
    UI->>API: Initialize chat
    API->>GPT: Generate opening message
    GPT-->>API: AI response text
    API-->>UI: Store message (hidden)
    UI->>TTS: Convert to speech
    TTS-->>UI: Play audio
    Note over UI: User listens...
    TTS-->>UI: Audio playback complete
    UI->>UI: Reveal message text
    
    User->>UI: Click record button
    User->>WebSpeech: Speak response
    WebSpeech-->>UI: Transcribed text
    User->>UI: Click send
    UI->>API: Send user message
    API->>GPT: Analyze + Generate reply
    GPT-->>API: Analysis + Response
    API-->>UI: Store reply (hidden)
    UI->>TTS: Play AI response audio
    Note over UI: User listens...
    TTS-->>UI: Audio playback complete
    UI->>UI: Reveal reply text + analysis button
```



#### Message Reveal States

```typescript
type MessageState = 
  | 'generating'    // AI is generating response
  | 'audio_loading' // TTS is converting text to audio
  | 'playing'       // Audio is playing, text hidden
  | 'revealed';     // Audio finished, text visible

interface ChatMessage {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  state: MessageState;  // Controls text visibility
  audioPlayed: boolean; // Tracks if user heard the message
}
```

#### UI Behavior

- **During 'playing' state**: Show audio waveform/progress indicator, text is hidden
- **After audio completes**: Smoothly reveal text with fade-in animation
- **Replay button**: Always available after first playback to listen again
- **Skip option** (optional setting): Allow advanced users to reveal text immediately

### 3. Message Analysis (GPT-powered)

- **Conversation fit**: How relevant is the response
- **Grammar check**: Errors and corrections
- **Vocabulary**: Word choice suggestions
- **Pronunciation hints**: Common mispronunciation patterns
- **Fluency score**: Overall assessment

### 4. Translation Features

- Click any word/phrase for translation (DeepL)
- Show usage examples (GPT-generated)
- Toggle full message translation
- Save words to vocabulary list

### 5. Settings System

**Language Settings:**

- Language to learn (English first)
- Dialect selection (American/British/Australian)
- Mother language (Ukrainian first)

**AI Provider Settings:**

- **AI Provider selection** (OpenAI/Ollama/WebLLM)
- **OpenAI API Mode** (Chat Completions / Assistants API) - only shown when OpenAI selected
- **AI Model selection** (per provider: gpt-4o-mini, llama3, etc.)

**Context & Summarization Settings:**

- **Recent window size** (default: 20) - messages kept in full
- **Summarize after** (default: 10) - trigger summarization every N messages outside window
- **Summarization provider** (Local / Same as chat) - use WebLLM/Ollama for free summarization
- **Local summarization model** - model for local summarization (e.g., Llama-3.2-1B)
- **Disable summarization** - toggle to always send full history

**Voice Settings:**

- **TTS Provider selection** (Kokoro/WebSpeech/OpenAI/ElevenLabs)
- Voice selection (per TTS provider)
- Speech speed control

**UI Settings:**

- **Listen-first mode** (default: ON) - text revealed only after audio playback

**Advanced Settings:**

- Customizable AI prompts (system prompt, analysis prompt, topic prompts)

## Project Structure

```
lanqua/
  src/
    app/
      page.tsx                 # Home/chat list
      chat/[id]/page.tsx       # Chat conversation
      settings/page.tsx        # Settings page
      api/
        chat/route.ts          # Chat API
        analyze/route.ts       # Analysis API
        translate/route.ts     # Translation API
    components/
      chat/
        ChatMessage.tsx        # Message bubble
        MessageAnalysis.tsx    # Analysis popup
        VoiceRecorder.tsx      # Recording component
        AudioPlayer.tsx        # Playback component
      ui/                      # shadcn components
      settings/
        LanguageSelector.tsx
        VoiceSelector.tsx
        AIProviderSelector.tsx # AI provider/model selection
        PromptEditor.tsx
    lib/
      db/
        schema.ts              # Drizzle schema
        index.ts               # DB connection
      tts/
        types.ts               # TTS provider interface
        manager.ts             # TTS provider manager
        providers/
          index.ts             # Provider registry
          kokoro.provider.ts   # Kokoro (local, WebGPU)
          webSpeech.provider.ts # Web Speech (local, fallback)
          openai.provider.ts   # OpenAI TTS (cloud)
      stt/
        types.ts               # STT provider interface
        manager.ts             # STT provider manager
        providers/
          webSpeech.provider.ts # Web Speech API
          whisper.provider.ts   # OpenAI Whisper (cloud)
      ai/
        types.ts               # AI provider interface
        manager.ts             # AI provider manager
        context.ts             # Conversation context manager (sliding window + summary)
        summarizer.ts          # Summarization engine with local/cloud fallback
        providers/
          index.ts             # Provider registry
          openai-chat.provider.ts    # OpenAI Chat Completions (we manage history)
          openai-assistant.provider.ts # OpenAI Assistants API (OpenAI manages history)
          ollama.provider.ts   # Ollama (local server) - Chat Completions only
          webllm.provider.ts   # WebLLM (local browser) - Chat Completions only
        prompts.ts             # System prompts builder
      translation/
        deepl.ts               # DeepL client
    stores/
      chatStore.ts             # Zustand store
      settingsStore.ts
      contextStore.ts          # Conversation context state
    types/
      index.ts                 # TypeScript types
  public/
  drizzle/                     # DB migrations
  package.json
  .env.local                   # API keys
```

## Database Schema

```sql
-- Chats table
CREATE TABLE chats (
  id TEXT PRIMARY KEY,
  title TEXT,
  topic_type TEXT,           -- 'general' | 'roleplay' | 'topic'
  topic_details TEXT,        -- JSON with topic config
  language TEXT,
  dialect TEXT,
  thread_id TEXT,            -- OpenAI Assistants API thread ID (null for other providers)
  ai_provider TEXT,          -- Provider used for this chat
  ai_mode TEXT,              -- 'chat' | 'assistant' (for OpenAI)
  created_at DATETIME,
  updated_at DATETIME
);

-- Messages table
CREATE TABLE messages (
  id TEXT PRIMARY KEY,
  chat_id TEXT REFERENCES chats(id),
  role TEXT,                 -- 'user' | 'assistant'
  content TEXT,
  audio_url TEXT,            -- cached audio blob URL
  analysis TEXT,             -- JSON with analysis data
  created_at DATETIME
);

-- Settings table (stores provider configs, prompts, preferences)
CREATE TABLE settings (
  key TEXT PRIMARY KEY,
  value TEXT                 -- JSON value
);
-- Example settings keys:
-- 'tts.provider' -> 'kokoro'
-- 'tts.voice' -> 'af_heart'
-- 'tts.speed' -> 1.0
-- 'ui.listenFirstMode' -> true (text hidden until audio finishes)
-- 'ai.provider' -> 'openai'
-- 'ai.openai.mode' -> 'chat' | 'assistant' (Chat Completions or Assistants API)
-- 'ai.openai.assistantId' -> 'asst_xxx' (stored when using Assistants API)
-- 'ai.model' -> 'gpt-4o-mini'
-- 'ai.prompts.system' -> 'You are a friendly language tutor...'
-- 'ai.prompts.analysis' -> 'Analyze the following response...'
-- 'context.recentWindowSize' -> 20 (messages kept in full)
-- 'context.summarizeAfterMessages' -> 10 (trigger summarization threshold)
-- 'context.summarizationProvider' -> 'local' | 'same'
-- 'context.localSummarizationModel' -> 'Llama-3.2-1B-Instruct'
-- 'context.disableSummarization' -> false
-- 'language.learning' -> 'en'
-- 'language.dialect' -> 'american'
-- 'language.mother' -> 'uk'

-- Vocabulary table
CREATE TABLE vocabulary (
  id TEXT PRIMARY KEY,
  word TEXT,
  translation TEXT,
  example TEXT,
  context TEXT,              -- from which chat
  created_at DATETIME
);

-- Chat summaries table (for context management)
CREATE TABLE chat_summaries (
  chat_id TEXT PRIMARY KEY REFERENCES chats(id),
  content TEXT,                    -- The summary text
  last_message_index INTEGER,      -- Index of last summarized message
  updated_at DATETIME
);
```

## Key Implementation Details

### Plugin Architecture

The system uses a provider-based plugin architecture for easy extensibility of TTS and AI engines.

#### TTS Provider Interface

```typescript
// lib/tts/types.ts
interface TTSProvider {
  id: string;
  name: string;
  type: 'local' | 'cloud';
  voices: Voice[];
  
  initialize(): Promise<void>;
  synthesize(text: string, options: TTSOptions): Promise<AudioBuffer>;
  getVoices(): Voice[];
  isAvailable(): boolean;
}

interface TTSOptions {
  voice: string;
  speed: number;      // 0.5 - 2.0
  pitch?: number;
  language: string;
  dialect?: string;
}
```

#### Built-in TTS Providers

```
lib/tts/
  providers/
    index.ts              # Provider registry
    kokoro.provider.ts    # Kokoro TTS (local, WebGPU)
    webSpeech.provider.ts # Web Speech Synthesis (local, fallback)
    openai.provider.ts    # OpenAI TTS (cloud)
    elevenlabs.provider.ts # ElevenLabs (cloud, optional)
  types.ts
  manager.ts              # TTS Manager with provider selection
```

Adding a new TTS provider:

1. Create `newProvider.provider.ts` implementing `TTSProvider`
2. Register in `providers/index.ts`
3. Provider appears automatically in settings

#### AI Provider Interface

```typescript
// lib/ai/types.ts
interface AIProvider {
  id: string;
  name: string;
  type: 'local' | 'cloud';
  contextMode: 'manual' | 'managed';  // Who manages conversation history
  models: AIModel[];
  
  initialize(): Promise<void>;
  chat(context: ConversationContext, message: string): Promise<AIResponse>;
  analyze(context: ConversationContext, userMessage: string): Promise<Analysis>;
  isAvailable(): boolean;
  
  // For 'managed' mode (Assistants API)
  createThread?(): Promise<string>;
  getThread?(threadId: string): Promise<ThreadMessages>;
}

interface AIOptions {
  model: string;
  temperature?: number;
  maxTokens?: number;
  systemPrompt: string;
}

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
  timestamp: Date;
}

// Context handling differs by provider mode
interface ConversationContext {
  chatId: string;
  threadId?: string;           // For Assistants API mode
  messages: ChatMessage[];     // For Chat Completions mode
  systemPrompt: string;
}
```

#### OpenAI Dual-Mode Support

OpenAI provider supports two API modes (user can switch in settings):


| Mode             | API                        | Context Management                     | Pros               | Cons                  |
| ---------------- | -------------------------- | -------------------------------------- | ------------------ | --------------------- |
| Chat Completions | `/v1/chat/completions`     | **We manage** - send full history      | Fast, full control | More tokens sent      |
| Assistants       | `/v1/assistants` + threads | **OpenAI manages** - just send new msg | Less data sent     | Async polling, slower |


```typescript
// OpenAI Chat Completions Mode (openai-chat.provider.ts)
class OpenAIChatProvider implements AIProvider {
  contextMode = 'manual';  // We send full history
  
  async chat(context: ConversationContext, message: string) {
    // Build messages array with ALL history
    const messages = [
      { role: 'system', content: context.systemPrompt },
      ...context.messages,
      { role: 'user', content: message }
    ];
    
    return await openai.chat.completions.create({
      model: this.model,
      messages  // Full history sent every time
    });
  }
}

// OpenAI Assistants Mode (openai-assistant.provider.ts)
class OpenAIAssistantProvider implements AIProvider {
  contextMode = 'managed';  // OpenAI stores history in Threads
  
  async chat(context: ConversationContext, message: string) {
    // Just add new message - OpenAI has the history
    await openai.beta.threads.messages.create(context.threadId, {
      role: 'user',
      content: message
    });
    
    // Run assistant (async - need to poll)
    const run = await openai.beta.threads.runs.create(context.threadId, {
      assistant_id: this.assistantId
    });
    
    return await this.waitForCompletion(run);
  }
  
  async createThread() {
    const thread = await openai.beta.threads.create();
    return thread.id;  // Store in chat record
  }
}
```

#### Built-in AI Providers

```
lib/ai/
  providers/
    index.ts                     # Provider registry
    openai-chat.provider.ts      # OpenAI Chat Completions (manual context)
    openai-assistant.provider.ts # OpenAI Assistants API (managed context)
    ollama.provider.ts           # Ollama (manual context only)
    webllm.provider.ts           # WebLLM (manual context only)
  types.ts
  manager.ts                     # AI Manager with provider selection
```


| Provider         | Context Mode       | Notes                                 |
| ---------------- | ------------------ | ------------------------------------- |
| OpenAI Chat      | Manual (we manage) | Default, fast, works offline analysis |
| OpenAI Assistant | Managed (OpenAI)   | Optional, OpenAI stores threads       |
| Ollama           | Manual only        | Local server, no managed option       |
| WebLLM           | Manual only        | Browser-based, no managed option      |


Adding a new AI provider:

1. Create `newProvider.provider.ts` implementing `AIProvider`
2. Set `contextMode` to `'manual'` (most providers) or `'managed'`
3. Register in `providers/index.ts`
4. Provider and its models appear in settings

### Conversation Context Management

The AI maintains conversation awareness through a context manager with **progressive summarization** to manage token costs while preserving context.

#### Context Strategy: Sliding Window + Summarization

```mermaid
flowchart TB
    subgraph FullHistory [All Messages in Database]
        M1[Msg 1-10]
        M2[Msg 11-20]
        M3[Msg 21-30]
        M4[Msg 31-40]
        M5[Msg 41-50 - Recent]
    end
    
    subgraph Summarizer [Summarization Engine]
        direction TB
        SumProvider[Summarization Provider]
        SumLocal[Local: WebLLM/Ollama - FREE]
        SumCloud[Cloud: Same as chat provider]
    end
    
    subgraph SentToAI [What Gets Sent to AI]
        Summary[Summary of Msg 1-30]
        Recent[Full Msg 31-50 - Recent Window]
        NewMsg[New User Message]
    end
    
    M1 --> Summarizer
    M2 --> Summarizer
    M3 --> Summarizer
    Summarizer --> Summary
    M4 --> Recent
    M5 --> Recent
    
    Summary --> AIProvider[AI Provider]
    Recent --> AIProvider
    NewMsg --> AIProvider
```



#### Configuration Options (User Settings)

```typescript
interface ContextSettings {
  // Sliding window size - messages kept in full
  recentWindowSize: number;        // Default: 20 messages
  
  // Summarization trigger
  summarizeAfterMessages: number;  // Default: 10 (summarize every 10 msgs outside window)
  
  // Summarization provider
  summarizationProvider: 'same' | 'local';  // 'same' = use chat provider, 'local' = WebLLM/Ollama
  localSummarizationModel?: string;         // e.g., 'Llama-3.2-1B-Instruct' for WebLLM
  
  // Disable summarization (send full history always)
  disableSummarization: boolean;   // Default: false
}
```

#### Summarization Flow

```mermaid
sequenceDiagram
    participant User
    participant ContextMgr as Context Manager
    participant DB as Database
    participant SumEngine as Summarization Engine
    participant AI as Chat AI Provider

    User->>ContextMgr: Send message (chat has 45 messages)
    ContextMgr->>DB: Load messages
    DB-->>ContextMgr: 45 messages
    
    Note over ContextMgr: Window=20, so 25 msgs need summarizing
    
    ContextMgr->>DB: Check existing summary
    DB-->>ContextMgr: Summary covers msgs 1-20
    
    Note over ContextMgr: Msgs 21-25 not summarized yet
    
    ContextMgr->>SumEngine: Summarize msgs 21-25
    Note over SumEngine: Uses local model if configured
    SumEngine-->>ContextMgr: New summary chunk
    
    ContextMgr->>ContextMgr: Merge summaries
    ContextMgr->>DB: Store updated summary
    
    ContextMgr->>AI: Send [Summary(1-25)] + [Full msgs 26-45] + [New msg]
    AI-->>ContextMgr: Response
```



#### Implementation

```typescript
// lib/ai/context.ts
class ContextManager {
  async buildContext(chatId: string, newMessage: string): Promise<AIMessages[]> {
    const settings = await getContextSettings();
    const allMessages = await db.getMessages(chatId);
    
    // If summarization disabled or few messages, send all
    if (settings.disableSummarization || allMessages.length <= settings.recentWindowSize) {
      return this.buildFullContext(allMessages, newMessage);
    }
    
    // Split: older messages (to summarize) vs recent (keep full)
    const splitIndex = allMessages.length - settings.recentWindowSize;
    const olderMessages = allMessages.slice(0, splitIndex);
    const recentMessages = allMessages.slice(splitIndex);
    
    // Get or create summary for older messages
    const summary = await this.getOrCreateSummary(chatId, olderMessages);
    
    // Build final context
    return [
      { role: 'system', content: this.buildSystemPrompt() },
      { role: 'system', content: `CONVERSATION SUMMARY (earlier messages):\n${summary}` },
      ...recentMessages.map(m => ({ role: m.role, content: m.content })),
      { role: 'user', content: newMessage }
    ];
  }
  
  private async getOrCreateSummary(chatId: string, messages: Message[]): Promise<string> {
    const existingSummary = await db.getChatSummary(chatId);
    const lastSummarizedIndex = existingSummary?.lastMessageIndex ?? 0;
    
    // Check if we need to update summary
    const unsummarizedMessages = messages.slice(lastSummarizedIndex);
    if (unsummarizedMessages.length < settings.summarizeAfterMessages) {
      return existingSummary?.content ?? '';
    }
    
    // Summarize new messages
    const newSummaryChunk = await this.summarize(unsummarizedMessages);
    
    // Merge with existing summary
    const mergedSummary = existingSummary 
      ? await this.mergeSummaries(existingSummary.content, newSummaryChunk)
      : newSummaryChunk;
    
    // Store updated summary
    await db.updateChatSummary(chatId, {
      content: mergedSummary,
      lastMessageIndex: messages.length
    });
    
    return mergedSummary;
  }
  
  private async summarize(messages: Message[]): Promise<string> {
    const settings = await getContextSettings();
    
    // Choose summarization provider
    const provider = settings.summarizationProvider === 'local'
      ? await this.getLocalSummarizer()  // WebLLM or Ollama
      : getActiveChatProvider();          // Same as chat
    
    const prompt = `Summarize this conversation segment concisely, preserving:
- Key topics discussed
- Important decisions or preferences expressed
- User's language patterns and common mistakes
- Any roleplay context or scenario details

CONVERSATION:
${messages.map(m => `${m.role}: ${m.content}`).join('\n')}

SUMMARY:`;
    
    return await provider.complete(prompt);
  }
  
  private async getLocalSummarizer(): Promise<AIProvider> {
    // Try WebLLM first (runs in browser)
    if (await webLLMProvider.isAvailable()) {
      return webLLMProvider;
    }
    // Fall back to Ollama if running
    if (await ollamaProvider.isAvailable()) {
      return ollamaProvider;
    }
    // Last resort: use chat provider
    return getActiveChatProvider();
  }
}
```

#### Database Schema for Summaries

```sql
-- Chat summaries table
CREATE TABLE chat_summaries (
  chat_id TEXT PRIMARY KEY REFERENCES chats(id),
  content TEXT,                    -- The summary text
  last_message_index INTEGER,      -- Index of last summarized message
  updated_at DATETIME
);
```

#### Summarization Provider Priority


| Setting                     | Provider Used               | Cost               |
| --------------------------- | --------------------------- | ------------------ |
| `local` + WebLLM available  | WebLLM (browser)            | FREE               |
| `local` + Ollama available  | Ollama (local server)       | FREE               |
| `local` + neither available | Falls back to chat provider | Paid               |
| `same`                      | Same as chat provider       | Paid (but simpler) |


#### Analysis with Summarized Context

```typescript
// Analysis also uses summary + recent for context
function buildAnalysisPrompt(
  summary: string | null,
  recentHistory: ChatMessage[], 
  userMessage: string
): string {
  return `
    You are analyzing a language learner's response in context.
    
    ${summary ? `EARLIER CONVERSATION SUMMARY:\n${summary}\n` : ''}
    
    RECENT CONVERSATION:
    ${recentHistory.map(m => `${m.role}: ${m.content}`).join('\n')}
    
    CURRENT MESSAGE TO ANALYZE:
    ${userMessage}
    
    Analyze considering:
    1. Grammar and vocabulary (with specific corrections)
    2. How well it fits the conversation flow
    3. Improvement compared to earlier messages (reference summary if available)
    4. Suggestions based on patterns you notice
  `;
}
```

#### Token Savings Example


| Conversation Length | Without Summary | With Summary (window=20)  |
| ------------------- | --------------- | ------------------------- |
| 30 messages         | ~6,000 tokens   | ~4,500 tokens (25% saved) |
| 50 messages         | ~10,000 tokens  | ~5,000 tokens (50% saved) |
| 100 messages        | ~20,000 tokens  | ~5,500 tokens (72% saved) |


### Kokoro TTS Integration

Kokoro runs entirely in the browser using WebGPU. Available voices:

- American English: af_heart (female), am_adam (male)
- British English: bf_emma (female), bm_george (male)

### Web Speech API for STT

```typescript
const recognition = new webkitSpeechRecognition();
recognition.continuous = true;
recognition.interimResults = true;
recognition.lang = 'en-US'; // or 'en-GB', 'en-AU'
```

### AI Analysis Prompt Structure

The GPT prompt will analyze user responses for:

1. Grammar correctness (errors highlighted)
2. Vocabulary appropriateness
3. Conversation relevance (0-100 score)
4. Suggestions for improvement
5. Alternative phrasings

## Additional Features (Suggested)

1. **Vocabulary Flashcards**: Spaced repetition for saved words
2. **Progress Tracking**: Stats on conversations, vocabulary learned
3. **Conversation Templates**: Pre-built scenarios for common situations
4. **Export/Import**: Backup chat history and settings
5. **Keyboard Shortcuts**: Quick record, send, play controls

## CONSIDERATIONS

- **Browser Compatibility**: Kokoro TTS and WebLLM require WebGPU (Chrome 113+, Edge 113+). Fallback providers available for older browsers
- **Web Speech API**: Limited to Chrome/Edge. Firefox has partial support
- **First Load**: Kokoro model (~80MB) and WebLLM models (1-4GB) need to download on first use
- **Context Length**: Full conversation history increases token usage with cloud AI providers. Consider summarization for very long chats (100+ messages)
- **Local AI Quality**: Ollama/WebLLM models may have lower quality than GPT-4o for analysis tasks

## RECOMMENDATIONS

1. **Start with MVP**: Focus on core chat + voice flow first, then add analysis features
2. **Consider Whisper API**: For users needing higher accuracy STT ($0.006/min)
3. **Add OpenAI TTS fallback**: For users where Kokoro quality is insufficient ($15/1M chars)
4. **PWA Support**: Add service worker for offline vocabulary review
5. **Context Summarization**: For long conversations (50+ messages), implement sliding window or summarization to manage token costs while maintaining context awareness
6. **Ollama Setup Guide**: Provide documentation for users wanting to run local AI with Ollama

