import { NextRequest, NextResponse } from 'next/server';
import { getChat, updateMessage, getMessage } from '@/lib/db/queries';
import { aiManager } from '@/lib/ai/manager';
import { contextManager } from '@/lib/ai/context';
import { buildSystemPrompt } from '@/lib/ai/prompts';
import { convertToWav } from '@/lib/audio/convert';

// CURSOR: Track last initialized provider to reinitialize when changed
let lastInitializedProvider: string | null = null;

// CURSOR: Initialize with specified provider and model, or use defaults
async function ensureInitialized(providerId?: string, model?: string) {
  const targetProvider = providerId || 'openai-chat';
  
  // CURSOR: Reinitialize if provider changed
  if (lastInitializedProvider !== targetProvider) {
    await aiManager.initialize(targetProvider);
    lastInitializedProvider = targetProvider;
  }
  
  // CURSOR: Set model if provided
  if (model) {
    aiManager.setModel(model);
  }
}

// POST /api/analyze - Analyze a user message
export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const { chatId, messageId, content, motherLanguage, aiProvider, aiModel } = body;
    
    // CURSOR: Initialize with provider/model from request (from user settings)
    await ensureInitialized(aiProvider, aiModel);

    if (!chatId || (!messageId && !content)) {
      return NextResponse.json(
        { error: 'chatId and either messageId or content required' },
        { status: 400 }
      );
    }

    const chat = await getChat(chatId);
    if (!chat) {
      return NextResponse.json({ error: 'Chat not found' }, { status: 404 });
    }

    // Get the content to analyze
    let textToAnalyze = content;
    let messageRecord = null as Awaited<ReturnType<typeof getMessage>> | null;
    if (messageId && !content) {
      const message = await getMessage(messageId);
      if (!message) {
        return NextResponse.json({ error: 'Message not found' }, { status: 404 });
      }
      messageRecord = message;
      textToAnalyze = message.content;
    }

    // CURSOR: Build context with safe JSON parsing
    let topicDetails: Record<string, unknown> = {};
    try {
      topicDetails = chat.topicDetails ? JSON.parse(chat.topicDetails) : {};
    } catch {
      console.warn('Failed to parse topicDetails for chat:', chatId);
    }
    // CURSOR: Get learning language from chat
    const learningLanguage = chat.language || 'en';
    
    const systemPrompt = buildSystemPrompt(
      chat.topicType as 'general' | 'roleplay' | 'topic',
      topicDetails.topicKey as string | undefined,
      undefined,
      learningLanguage
    );
    
    const fullContext = await contextManager.buildContext(chatId, systemPrompt, chat.threadId || undefined);

    // CURSOR: Filter context to only include messages BEFORE the message being analyzed
    // This ensures the AI sees what the user was responding to, not what came after
    let context = fullContext;
    if (messageId && messageRecord) {
      const messageTimestamp = messageRecord.createdAt?.getTime() || 0;
      const filteredMessages = fullContext.messages.filter(m => {
        // Keep messages that came before the analyzed message
        // We compare by content since we don't have timestamps in ChatMessage
        return true; // Will filter by index below
      });
      
      // Find index of the message being analyzed and keep only messages before it
      const messageIndex = fullContext.messages.findIndex(m => m.content === textToAnalyze && m.role === 'user');
      if (messageIndex > 0) {
        context = {
          ...fullContext,
          messages: fullContext.messages.slice(0, messageIndex),
        };
        console.log('[Analyze API] Filtered context to', context.messages.length, 'messages (before analyzed message)');
      }
    }

    // CURSOR: Perform analysis with both languages
    // If audio is available, pass it to analyze() for unified analysis (one AI call)
    // The AI will listen to the audio, transcribe, and analyze grammar/vocabulary/relevance/pronunciation together
    console.log('[Analyze API] Starting analysis for:', textToAnalyze.substring(0, 50) + '...');
    console.log('[Analyze API] Mother language:', motherLanguage || 'uk', 'Learning language:', learningLanguage);
    
    let audioBase64: string | undefined;
    let audioFormatStr: string | undefined;
    
    if (messageRecord?.audioBlob) {
      try {
        const originalFormat = messageRecord.audioFormat || 'webm';
        console.log('[Analyze API] Converting audio from', originalFormat, 'to wav for unified analysis');
        
        const wavBuffer = await convertToWav(Buffer.from(messageRecord.audioBlob), originalFormat);
        audioBase64 = wavBuffer.toString('base64');
        audioFormatStr = 'wav';
        
        console.log('[Analyze API] Audio converted, size:', wavBuffer.length, 'bytes');
      } catch (error) {
        console.error('[Analyze API] Audio conversion failed, falling back to text-only:', error);
      }
    }

    // CURSOR: Single unified call - if audio is present, AI analyzes audio + text + pronunciation together
    const analysis = await aiManager.analyze(context, textToAnalyze, { 
      motherLanguage: motherLanguage || 'uk',
      learningLanguage: learningLanguage,
      audioBase64,
      audioFormat: audioFormatStr,
    });
    
    console.log('[Analyze API] Analysis result keys:', Object.keys(analysis));
    console.log('[Analyze API] Has pronunciation:', !!analysis.pronunciation);

    // If messageId provided, save analysis to message
    if (messageId) {
      await updateMessage(messageId, {
        analysis: JSON.stringify(analysis),
      });
    }

    return NextResponse.json({ analysis });
  } catch (error) {
    console.error('Analyze error:', error);
    return NextResponse.json(
      { error: 'Failed to analyze message' },
      { status: 500 }
    );
  }
}
