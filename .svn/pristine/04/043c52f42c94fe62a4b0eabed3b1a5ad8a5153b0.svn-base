import type { AIProvider, AIModel, AIResponse, Analysis, UnifiedResponse, ConversationContext, AIOptions, AIProviderStatus, RichTranslation } from '../types';
import { getRichTranslationPrompt, getUnifiedResponsePrompt } from '../prompts';

/**
 * @deprecated Ollama provider is not actively used. Audio-based flow requires OpenAI.
 * Kept for compilation but not recommended for use.
 */

export class OllamaProvider implements AIProvider {
  id = 'ollama';
  name = 'Ollama (Local)';
  type = 'local' as const;
  contextMode = 'manual' as const;
  
  models: AIModel[] = [
    { id: 'llama3.2', name: 'Llama 3.2', contextWindow: 128000, description: 'Latest Llama model' },
    { id: 'llama3.1', name: 'Llama 3.1', contextWindow: 128000, description: 'Previous Llama model' },
    { id: 'mistral', name: 'Mistral', contextWindow: 32000, description: 'Fast and capable' },
    { id: 'gemma2', name: 'Gemma 2', contextWindow: 8192, description: 'Google Gemma model' },
    { id: 'qwen2.5', name: 'Qwen 2.5', contextWindow: 128000, description: 'Alibaba Qwen model' },
  ];

  private baseUrl: string = 'http://localhost:11434';
  private status: AIProviderStatus = {
    initialized: false,
    loading: false,
  };

  async initialize(): Promise<void> {
    // CURSOR: Allow custom Ollama URL via environment variable
    this.baseUrl = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
    
    // Check if Ollama is running by fetching available models
    try {
      this.status.loading = true;
      const response = await fetch(`${this.baseUrl}/api/tags`, {
        method: 'GET',
      });

      if (response.ok) {
        const data = await response.json();
        // CURSOR: Update models list with actually available models
        if (data.models && Array.isArray(data.models)) {
          const availableModels = data.models.map((m: { name: string }) => ({
            id: m.name,
            name: m.name,
            contextWindow: 32000, // Default context window
            description: 'Locally installed model',
          }));
          
          if (availableModels.length > 0) {
            this.models = availableModels;
          }
        }
        
        this.status = {
          initialized: true,
          loading: false,
        };
      } else {
        this.status = {
          initialized: false,
          loading: false,
          error: 'Failed to connect to Ollama server',
        };
      }
    } catch (error) {
      this.status = {
        initialized: false,
        loading: false,
        error: 'Ollama server not available. Make sure Ollama is running.',
      };
    }
  }

  async generate(context: ConversationContext, message: string, options?: AIOptions): Promise<AIResponse> {
    const model = options?.model || this.models[0]?.id || 'llama3.2';
    
    const messages: Array<{ role: 'system' | 'user' | 'assistant'; content: string }> = [
      { role: 'system', content: context.systemPrompt },
    ];

    if (context.summary) {
      messages.push({
        role: 'system',
        content: `CONVERSATION SUMMARY (earlier messages):\n${context.summary}`,
      });
    }

    messages.push(...context.messages);
    messages.push({ role: 'user', content: message });

    try {
      const response = await fetch(`${this.baseUrl}/api/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model,
          messages,
          stream: false,
          options: {
            temperature: options?.temperature ?? 0.7,
            num_predict: options?.maxTokens ?? 500,
          },
        }),
      });

      if (!response.ok) {
        throw new Error(`Ollama API error: ${response.status}`);
      }

      const data = await response.json();
      return {
        content: data.message?.content || '',
        usage: data.eval_count ? {
          promptTokens: data.prompt_eval_count || 0,
          completionTokens: data.eval_count || 0,
          totalTokens: (data.prompt_eval_count || 0) + (data.eval_count || 0),
        } : undefined,
      };
    } catch (error) {
      throw new Error(`Ollama generate failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  async respond(context: ConversationContext, userMessage: string, options?: AIOptions): Promise<UnifiedResponse> {
    const model = options?.model || this.models[0]?.id || 'llama3.2';
    const unifiedPrompt = getUnifiedResponsePrompt(options?.motherLanguage, options?.learningLanguage, false);
    
    const recentMessages = context.messages.slice(-10);
    const conversationContext = `${context.summary ? `EARLIER CONVERSATION SUMMARY:\n${context.summary}\n\n` : ''}RECENT CONVERSATION:\n${recentMessages.map(m => `${m.role}: ${m.content}`).join('\n')}`;

    try {
      const response = await fetch(`${this.baseUrl}/api/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model,
          messages: [
            { role: 'system', content: `${unifiedPrompt}\n\nRespond ONLY with valid JSON, no other text.` },
            { role: 'user', content: `${conversationContext}\n\nMESSAGE TO RESPOND TO AND ANALYZE:\n${userMessage}` },
          ],
          stream: false,
          format: 'json',
          options: {
            temperature: 0.3,
            num_predict: options?.maxTokens ?? 2000,
          },
        }),
      });

      if (!response.ok) {
        throw new Error(`Ollama API error: ${response.status}`);
      }

      const data = await response.json();
      const content = data.message?.content || '{}';

      try {
        const parsed = JSON.parse(content);
        return {
          reply: parsed.reply ?? '',
          analysis: {
            grammarScore: parsed.analysis?.grammarScore ?? 70,
            grammarErrors: Array.isArray(parsed.analysis?.grammarErrors) ? parsed.analysis.grammarErrors : [],
            vocabularyScore: parsed.analysis?.vocabularyScore ?? 70,
            vocabularySuggestions: Array.isArray(parsed.analysis?.vocabularySuggestions) ? parsed.analysis.vocabularySuggestions : [],
            relevanceScore: parsed.analysis?.relevanceScore ?? 80,
            relevanceFeedback: parsed.analysis?.relevanceFeedback,
            overallFeedback: parsed.analysis?.overallFeedback ?? 'Good effort!',
            alternativePhrasings: Array.isArray(parsed.analysis?.alternativePhrasings) ? parsed.analysis.alternativePhrasings : [],
          },
        };
      } catch {
        return this.getDefaultUnifiedResponse();
      }
    } catch (error) {
      console.error('Ollama respond failed:', error);
      return this.getDefaultUnifiedResponse();
    }
  }

  private getDefaultUnifiedResponse(): UnifiedResponse {
    return {
      reply: '',
      analysis: {
        grammarScore: 70,
        grammarErrors: [],
        vocabularyScore: 70,
        vocabularySuggestions: [],
        relevanceScore: 80,
        overallFeedback: 'Unable to perform detailed analysis.',
        alternativePhrasings: [],
      },
    };
  }

  // CURSOR: Rich translation with definition, usage examples, and type classification
  async richTranslate(
    text: string,
    learningLanguage: string,
    motherLanguage: string,
    options?: AIOptions
  ): Promise<RichTranslation> {
    const model = options?.model || this.models[0]?.id || 'llama3.2';
    const prompt = getRichTranslationPrompt(text, learningLanguage, motherLanguage);

    try {
      const response = await fetch(`${this.baseUrl}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model,
          messages: [
            { 
              role: 'system', 
              content: 'You are a language learning assistant providing detailed translations. Respond ONLY with valid JSON.' 
            },
            { role: 'user', content: prompt },
          ],
          stream: false,
          format: 'json',
          options: {
            temperature: 0.3,
            num_predict: options?.maxTokens ?? 500,
          },
        }),
      });

      if (!response.ok) {
        throw new Error(`Ollama API error: ${response.status}`);
      }

      const data = await response.json();
      const content = data.message?.content || '{}';
      
      try {
        const parsed = JSON.parse(content);
        return {
          translation: parsed.translation || text,
          type: parsed.type || 'word',
          definition: parsed.definition || '',
          usageExamples: Array.isArray(parsed.usageExamples) ? parsed.usageExamples : [],
          notes: parsed.notes || undefined,
          formality: parsed.formality || 'neutral',
        };
      } catch {
        return this.getDefaultRichTranslation(text);
      }
    } catch (error) {
      console.error('Ollama richTranslate failed:', error);
      return this.getDefaultRichTranslation(text);
    }
  }

  private getDefaultRichTranslation(text: string): RichTranslation {
    return {
      translation: text,
      type: 'word',
      definition: '',
      usageExamples: [],
      formality: 'neutral',
    };
  }

  async isAvailable(): Promise<boolean> {
    try {
      const response = await fetch(`${this.baseUrl}/api/tags`, {
        method: 'GET',
        signal: AbortSignal.timeout(2000), // 2 second timeout
      });
      return response.ok;
    } catch {
      return false;
    }
  }

  getStatus(): AIProviderStatus {
    return this.status;
  }
}

// Export singleton instance
export const ollamaProvider = new OllamaProvider();
